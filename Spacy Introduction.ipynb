{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "token = doc[0]\n",
    "\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Span object is a slice of the document consisting of one or more tokens. It's only a view of the Doc and doesn't contain any data itself.\n",
    "\n",
    "To create a Span, you can use Python's slice notation. For example, 1 colon 3 will create a slice starting from the token at position 1, up to – but not including! – the token at position 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see some of the available token attributes:\n",
    "\n",
    "\"i\" is the index of the token within the parent document.\n",
    "\n",
    "\"text\" returns the token text.\n",
    "\n",
    "\"is alpha\", \"is punct\" and \"like num\" return boolean values indicating whether the token consists of alphabetic characters, whether it's punctuation or whether it resembles a number. For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
    "\n",
    "These attributes are also called lexical attributes: they refer to the entry in the vocabulary and don't depend on the token's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :  [0, 1, 2, 3, 4, 5]\n",
      "Text :  ['This', 'will', 'cost', '$', '5', '.']\n",
      "Is Alpha :  [True, True, True, False, False, False]\n",
      "Is Punct :  [False, False, False, False, False, True]\n",
      "Like Num :  [False, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This will cost $5.\")\n",
    "\n",
    "print('Index : ', [token.i for token in doc])\n",
    "print('Text : ', [token.text for token in doc])\n",
    "print('Is Alpha : ', [token.is_alpha for token in doc])\n",
    "print('Is Punct : ', [token.is_punct for token in doc])\n",
    "print('Like Num : ', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are statistical models ?\n",
    "\n",
    "-> Enable Spacy to predict linguistic attributes in context\n",
    "    1. Part-of-speech tags\n",
    "    2. Syntactic dependencies\n",
    "    3. Names entities\n",
    "-> Trained on labeled example texts\n",
    "-> Can be updated with more examples to fine-tune predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Packages : \n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Binary weights\n",
    "# Vocabolary\n",
    "# Meta Information(language, pipeline)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s not included in a model package that you can load into spaCy?\n",
    "\n",
    "    a.A meta file including the language, pipeline and license.\n",
    "            All models include a meta.json that defines the language to initialize, the pipeline component names to load as well as general meta information like the model name, version, license, data sources, author and accuracy figures (if available).\n",
    "    b.Binary weights to make statistical predictions.\n",
    "            To predict linguistic annotations like part-of-speech tags, dependency labels or named entities, models include binary weights.\n",
    "    c.The labelled data that the model was trained on. (This is not included)\n",
    "            Statistical models allow you to generalize based on a set of training examples. Once they’re trained, they use binary weights to make predictions. That’s why it’s not necessary to ship them with their training data.\n",
    "    d.Strings of the model's vocabulary and their hashes.\n",
    "            Model packages include a strings.json that stores the entries in the model’s vocabulary and the mapping to hashes. This allows spaCy to only communicate in hashes and look up the corresponding string if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are statistical and not always right. Whether their predictions are correct depends on the training data and the text you’re processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just regular expressions ?\n",
    "\n",
    "1. Match on Doc objects, not just strings\n",
    "2. Match on tokens and token attributes\n",
    "3. Use the model's predictions\n",
    "4. Example: \"duck\"(verb) vs. \"duck\"(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lists of dictionaries, one per token\n",
    "2. Match exact token texts\n",
    "\n",
    "        [{'TEXT': 'iPhone'} {'TEXT': 'X'}]\n",
    "\n",
    "3. Match lexical attributes\n",
    "        [{'LOWER' : 'iphone'},{'LOWER' : 'x'}]\n",
    "\n",
    "4. Match any token attributes\n",
    "        [{'LOWER': 'buy'}, {'POS', : 'NOUN'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Matcher Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the Matcher\n",
    "pattern = [{'TEXT':'iPhone'}, {'TEXT':'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp('New iPhone X release data leaked')\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Matcher Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "#Iterate over the matches\n",
    "# match_id : hash value ot the pattern name\n",
    "# start : start index of matched span\n",
    "# end : end index of matched span\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern =[\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11920309760829426267, 0, 5)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN', None, pattern)\n",
    "\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    match_span = doc[start:end]\n",
    "    print(match_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching other token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS' : 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now i love cats more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5725346615152885079, 1, 3), (5725346615152885079, 6, 8)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('TOKEN_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    match_span = doc[start:end]\n",
    "    print(match_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using operators and quantifiers Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14539848206026242071, 1, 4), (14539848206026242071, 8, 10)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'buy'},\n",
    "    {'POS': 'DET', 'OP':'?'},    # Optional : match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('OPERAOTRS', None, pattern)\n",
    "\n",
    "matches= matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    match_span = doc[start:end]\n",
    "    print(match_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using operators and quantifiers Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example                 Description\n",
    "\n",
    "\n",
    "#{'OP': '!'}             Negation: match 0 times\n",
    "#{'OP': '?'}             Optional: match 0 or 1 times\n",
    "#{'OP': '+'}             Match 1 or more times\n",
    "#{'OP': '*'}             Match 0 or more times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Match Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
